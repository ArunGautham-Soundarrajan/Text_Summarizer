{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade tensorflow-datasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom bs4 import BeautifulSoup \n\nimport re\nfrom nltk.corpus import stopwords \n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-24T11:31:06.442086Z","iopub.execute_input":"2021-07-24T11:31:06.442426Z","iopub.status.idle":"2021-07-24T11:31:06.453519Z","shell.execute_reply.started":"2021-07-24T11:31:06.442392Z","shell.execute_reply":"2021-07-24T11:31:06.451290Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"ds = tfds.load('reddit_tifu')\ndf = tfds.as_dataframe(ds['train'].take(30000))\n\n#We only need these two columns for summarization\ndf = df[['documents','title']]\n\n# Convert object to string\ndf = df.astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T10:16:14.281468Z","iopub.execute_input":"2021-07-24T10:16:14.281822Z","iopub.status.idle":"2021-07-24T10:16:26.337418Z","shell.execute_reply.started":"2021-07-24T10:16:14.281786Z","shell.execute_reply":"2021-07-24T10:16:26.336506Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"#Remove the first two characters\ndf['documents'] = df.documents.str.slice_replace(stop=2, repl ='')\ndf['title'] = df.title.str.slice_replace(stop=2, repl ='')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T10:16:26.339015Z","iopub.execute_input":"2021-07-24T10:16:26.339386Z","iopub.status.idle":"2021-07-24T10:16:26.423428Z","shell.execute_reply.started":"2021-07-24T10:16:26.339345Z","shell.execute_reply":"2021-07-24T10:16:26.422577Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"#Contraction mapping\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","metadata":{"execution":{"iopub.status.busy":"2021-07-24T10:14:05.280077Z","iopub.execute_input":"2021-07-24T10:14:05.280505Z","iopub.status.idle":"2021-07-24T10:14:05.307445Z","shell.execute_reply.started":"2021-07-24T10:14:05.280467Z","shell.execute_reply":"2021-07-24T10:14:05.305963Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"#get stopwords from NLTK\nstop_words = set(stopwords.words('english')) \n\n#this function cleans the text\ndef text_cleaner(text):\n    \"\"\"\n    This function cleanes the texts and returns the cleaned senctance.\n    \n    Input:\n    text - Any string\n    \n    Output:\n    Sentance - Cleaned string\n    \"\"\"\n    \n    newString = text.lower()\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)\n    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    long_words=[]\n    \n    for i in tokens:\n        if len(i)>=3:                  #removing short word\n            long_words.append(i)   \n            \n    return (\" \".join(long_words)).strip()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T10:14:48.187304Z","iopub.execute_input":"2021-07-24T10:14:48.187655Z","iopub.status.idle":"2021-07-24T10:14:48.195776Z","shell.execute_reply.started":"2021-07-24T10:14:48.187623Z","shell.execute_reply":"2021-07-24T10:14:48.194768Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#Applying the functions to clean each column\ndf.documents = df.documents.apply(text_cleaner)\ndf.title = df.title.apply(text_cleaner)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T10:16:26.425277Z","iopub.execute_input":"2021-07-24T10:16:26.425653Z","iopub.status.idle":"2021-07-24T10:16:35.750332Z","shell.execute_reply.started":"2021-07-24T10:16:26.425615Z","shell.execute_reply":"2021-07-24T10:16:35.749338Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"df.title = df.title.apply(lambda x : '_START_ '+ x + ' _END_')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:59:50.843932Z","iopub.execute_input":"2021-07-24T11:59:50.844335Z","iopub.status.idle":"2021-07-24T11:59:50.866381Z","shell.execute_reply.started":"2021-07-24T11:59:50.844294Z","shell.execute_reply":"2021-07-24T11:59:50.865469Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing","metadata":{}},{"cell_type":"code","source":"# Maximum length for each input and target \nmax_text_len = 120\nmax_summary_len = 15","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:00:25.532719Z","iopub.execute_input":"2021-07-24T12:00:25.533104Z","iopub.status.idle":"2021-07-24T12:00:25.538019Z","shell.execute_reply.started":"2021-07-24T12:00:25.533070Z","shell.execute_reply":"2021-07-24T12:00:25.537074Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"#Train and test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.documents, df.title, test_size = 0.1, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:00:26.890822Z","iopub.execute_input":"2021-07-24T12:00:26.891194Z","iopub.status.idle":"2021-07-24T12:00:26.904610Z","shell.execute_reply.started":"2021-07-24T12:00:26.891158Z","shell.execute_reply":"2021-07-24T12:00:26.903751Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def sequence_tokenizer(train, test, max_len):\n    \"\"\"\n    This function tokenizes and adds padding to a sequence of texts.\n    \n    Input:\n    train - Train data which contains sentances.\n    test - Test data, which also contains sentances.\n    max_len - Argument to specify the maximum length for padding.\n    \n    Output:\n    train - Toeknized train data\n    test - Tokenized test data\n    tokenizer - The tokenizer instance ( To invert back in the future )\n    \"\"\"\n    \n    #Initialize the tokenizer\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(train)\n    \n    #Texts to integer squences\n    train = tokenizer.texts_to_sequences(train)\n    test = tokenizer.texts_to_sequences(test)\n    \n    #Add padding\n    train = pad_sequences(train, maxlen= max_len, padding = 'post')\n    test = pad_sequences(test, maxlen= max_len, padding = 'post')\n    \n    return train, test, tokenizer\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:00:27.811890Z","iopub.execute_input":"2021-07-24T12:00:27.812248Z","iopub.status.idle":"2021-07-24T12:00:27.819124Z","shell.execute_reply.started":"2021-07-24T12:00:27.812213Z","shell.execute_reply":"2021-07-24T12:00:27.818023Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, X_tokenizer = sequence_tokenizer(X_train, X_test, max_text_len)\ny_train, y_test, y_tokenizer = sequence_tokenizer(y_train, y_test, max_summary_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:00:29.002357Z","iopub.execute_input":"2021-07-24T12:00:29.002706Z","iopub.status.idle":"2021-07-24T12:00:35.261851Z","shell.execute_reply.started":"2021-07-24T12:00:29.002672Z","shell.execute_reply":"2021-07-24T12:00:35.260929Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}